# -*- coding: utf-8 -*-
"""part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PZDUmzRfCLZuMPXkGNoKJKQozkZF6lSq

Team members:
Mahathir Maxim - mhm180000
Edwin Betancourt - exb150030
"""

#importing necessary libraries

import numpy as np
  
import pandas as pd
  
from sklearn.model_selection import train_test_split
  
import matplotlib.pyplot as plt

import seaborn as sns 

from sklearn.metrics import r2_score

from sklearn.metrics import mean_squared_error

def attributeTuning(X):

    #This function performs normalization on the passed dataset
    meanVal = np.mean(X, axis = 0)  
    stdDev = np.std(X, axis= 0, ddof = 1)  
    X_updated= X - meanVal
    X_normalized = X_updated/stdDev
    

    return X_normalized

def errorFunc(X, Y, coef):

    #this function calculates the error on the passed data 
    guess = X.dot(coef)
    err = np.subtract(guess, Y)
    leng= len(Y)
    sqrErr = np.square(err) 
    returnVal = err.T.dot(err)
    returnVal=1/(2 * leng) * returnVal

    return returnVal

def gradientDescent(X, Y, coef, rate, noIter):
    
    #This function finds calculates the gradient descent and updates the coeffecients until inds the ooptimal value
    hist = np.zeros(noIter)

    for i in range(noIter):
        guess = X.dot(coef)
        err = np.subtract(guess, Y)
        m=len(Y)
        sum =  X.transpose().dot(err);
        sum = (rate / m) * sum
        coef = coef - sum;
        hist[i] = errorFunc(X, Y, coef)  

    return coef, hist

def main() :
      
    dataset = pd.read_csv( "https://cs4375fall21.s3.amazonaws.com/RealEstateData.csv" ) #imorting the dataset
    dataset.isnull().sum() # checking if data has any null values 
    correlation= dataset.corr().round(2) # checking correlation between different attributes
    print(correlation)
    sns.heatmap(data=correlation, annot=True) # mapping the correlations between attributes
    dataset.drop('No', axis=1, inplace=True)  # dropping irrelevant attributes
    dataset.drop('X1 transaction date', axis=1, inplace=True)
    correlation= dataset.corr().round(2)
    #sns.heatmap(data=correlation, annot=True)
    sns.set(rc={'figure.figsize':(11.7,8.27)})
    sns.distplot(dataset['Y house price of unit area'], bins=30)
    plt.show()
    #assigning X and Y
    X = dataset.values[:, 0:5]  
    y = dataset.values[:, 5]  
    leng = len(y)
    
    #normalizing the data
    X = attributeTuning(X)

    X = np.hstack((np.ones((leng,1)), X))

    #splitting the data 80% training and 20% testing
    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)
    print(X_train.shape)
    print(X_test.shape)
    print(Y_train.shape)
    print(Y_test.shape)
    

    theta = np.zeros(6)
    noIter = 1500; #number of iterations
    rate = 0.5; #learning rate

    #performing gradient descent on the training data to get thetas and cost history over iterations
    theta, cost_history = gradientDescent(X_train, Y_train, theta, rate, noIter)
    print('Final value of theta =', theta)

    # getting measurement variable R^2 and RMSE 
    y_train_predict= X_train.dot(theta)
    rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))
    r2 = r2_score(Y_train, y_train_predict)

    print("Training set")
    print("--------------------------------------")
    print('RMSE = {}'.format(rmse))
    print('R2 = {}'.format(r2))
    print("\n")

    
    plt.plot(range(1, noIter +1), cost_history, color ='blue')
    plt.rcParams["figure.figsize"] = (5,2)
    plt.grid()
    plt.ylabel("Error")
    plt.xlabel("Number of iterations")
    plt.title("Convergence plot of gradient descent (training)")
    
    
    #performing gradient descent on test data 
    theta, cost_history_test = gradientDescent(X_test, Y_test, theta, rate, noIter)
    print('Final value of theta =', theta)

    #getting R^2 and RMSE for test data
    y_test_predict= X_test.dot(theta)
    rmse_test = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))
    r2_test = r2_score(Y_test, y_test_predict)

    print("Test set")
    print("--------------------------------------")
    print('RMSE = {}'.format(rmse_test))
    print('R2 = {}'.format(r2_test))
    print("\n")

    # plotting the cost vs iteration data
    plt.plot(range(1, noIter +1), cost_history_test, color ='red')
    plt.rcParams["figure.figsize"] = (5,2)
    plt.grid()
    plt.ylabel("Error)")
    plt.xlabel("Number of iterations")
    plt.title("Convergence plot of gradient descent (test)")
    
     
if __name__ == "__main__" : 
      
    main()